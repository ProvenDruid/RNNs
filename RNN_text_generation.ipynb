{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMo3YevndyaOZvfdY7RNM95",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ProvenDruid/RNNs/blob/main/RNN_text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhHOo7R_WfkX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/sample_data/shakespeare.txt','r',encoding='utf8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "wT8fOLiGYt17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])\n"
      ],
      "metadata": {
        "id": "vcOC43Z5dinJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "114617a8-46b9-4b7f-bfae-cf8e17b29a96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "                     1\n",
            "  From fairest creatures we desire increase,\n",
            "  That thereby beauty's rose might never die,\n",
            "  But as the riper should by time decease,\n",
            "  His tender heir might bear his memory:\n",
            "  But thou contracted to thine own bright eyes,\n",
            "  Feed'st thy light's flame with self-substantial fuel,\n",
            "  Making a famine where abundance lies,\n",
            "  Thy self thy foe, to thy sweet self too cruel:\n",
            "  Thou that art now the world's fresh ornament,\n",
            "  And only herald to the gaudy spring,\n",
            "  Within thine own bud buriest thy content,\n",
            "  And tender churl mak'st waste in niggarding:\n",
            "    Pity the world, or else this glutton be,\n",
            "    To eat the world's due, by the grave and thee.\n",
            "\n",
            "\n",
            "                     2\n",
            "  When forty winters shall besiege thy brow,\n",
            "  And dig deep trenches in thy beauty's field,\n",
            "  Thy youth's proud livery so gazed on now,\n",
            "  Will be a tattered weed of small worth held:  \n",
            "  Then being asked, where all thy beauty lies,\n",
            "  Where all the treasure of thy lusty days;\n",
            "  To say within thine own deep su\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrPgbE2UZUVl",
        "outputId": "eb292737-04db-4ed7-8cbc-b1413ace9f60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5445609"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#first step is to encode text, in this case by character\n",
        "all_chars = set(text)"
      ],
      "metadata": {
        "id": "nPItgvzyZckd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQpB0rNeZk-G",
        "outputId": "950b67cb-b6b6-4493-f2e2-7d19bb3af461"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "84"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#then we need to create like a look up table, with a dict\n",
        "decoder = dict(enumerate(all_chars))\n",
        "list(decoder.items())[:30]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbRWPl6cZ0x5",
        "outputId": "d4bf309a-c99a-4a08-ede2-ee4609ef4cf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 'q'),\n",
              " (1, 'L'),\n",
              " (2, '|'),\n",
              " (3, 'o'),\n",
              " (4, '4'),\n",
              " (5, 'b'),\n",
              " (6, '?'),\n",
              " (7, '&'),\n",
              " (8, 'X'),\n",
              " (9, '<'),\n",
              " (10, '8'),\n",
              " (11, 'P'),\n",
              " (12, '0'),\n",
              " (13, 'v'),\n",
              " (14, 'Q'),\n",
              " (15, 'n'),\n",
              " (16, 's'),\n",
              " (17, 'u'),\n",
              " (18, 'I'),\n",
              " (19, 'D'),\n",
              " (20, 'N'),\n",
              " (21, 'g'),\n",
              " (22, 'd'),\n",
              " (23, '>'),\n",
              " (24, \"'\"),\n",
              " (25, 'W'),\n",
              " (26, ':'),\n",
              " (27, '['),\n",
              " (28, ')'),\n",
              " (29, 'G')]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#then exactly the opposite for the decoder\n",
        "encoder = {char: ind for ind,char in decoder.items()}\n",
        "#and fially we encode the text\n",
        "encoded_txt = np.array([encoder[char] for char in text])"
      ],
      "metadata": {
        "id": "aB8Zt3xrayKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_txt[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ca08najbCzN",
        "outputId": "3f8dd066-9e5f-4bb1-c27a-3f88dd8c7477"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([41, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35,\n",
              "       35, 35, 35, 35, 35, 67, 41, 35, 35, 58, 78,  3, 53, 35, 59, 70, 66,\n",
              "       78, 75, 16, 60, 35, 55, 78, 75, 70, 60, 17, 78, 75, 16, 35, 56, 75,\n",
              "       35, 22, 75, 16, 66, 78, 75, 35, 66, 15, 55, 78, 75, 70, 16, 75, 47,\n",
              "       41, 35, 35, 65, 44, 70, 60, 35, 60, 44, 75, 78, 75,  5, 71, 35,  5,\n",
              "       75, 70, 17, 60, 71, 24, 16, 35, 78,  3, 16, 75, 35, 53, 66])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#now we do a one hot encoding\n",
        "def one_hot_encoder(encoded_text, num_uni_chars):\n",
        "    #where:\n",
        "    #encoded_text : batch of encoded text\n",
        "    #num_uni_chars = number of unique characters (len(set(text)))\n",
        "\n",
        "\n",
        "    # METHOD FROM:\n",
        "    # https://stackoverflow.com/questions/29831489/convert-encoded_textay-of-indices-to-1-hot-encoded-numpy-encoded_textay\n",
        "\n",
        "    # first create a placeholder for zeros.\n",
        "    one_hot = np.zeros((encoded_text.size, num_uni_chars))\n",
        "\n",
        "    # Convert data type for later because otherwise I got erros\n",
        "    one_hot = one_hot.astype(np.float32)\n",
        "\n",
        "    # now we put a 1 in the corresponding location of the index\n",
        "    one_hot[np.arange(one_hot.shape[0]), encoded_text.flatten()] = 1.0\n",
        "\n",
        "\n",
        "    # Reshape it so it matches the batch sahe\n",
        "    one_hot = one_hot.reshape((*encoded_text.shape, num_uni_chars))\n",
        "\n",
        "    return one_hot"
      ],
      "metadata": {
        "id": "3UZshVzWbmeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing the function\n",
        "one_hot_encoder(np.array([1,2,0]),3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGBvfV13cVCt",
        "outputId": "1287f786-8d88-4ae6-b403-c097a0d7f830"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_batch(encoded_text, samp_per_batch=10, seq_len=50):\n",
        "\n",
        "    '''\n",
        "\n",
        "    train_data: Encoded Text of length seq_len\n",
        "    labels: Encoded Text shifted by one char\n",
        "    for example if the train data is\n",
        "    train : [[1 2 3]]\n",
        "    then the label (the target) will be\n",
        "    labels:[[2 3 4]]\n",
        "\n",
        "    so, the idea is that given a piece of text, the model will try to predict whats the next\n",
        "    thing you wanted to say, according to what he learned during training,\n",
        "    but since the dataset is quite small It might give nonsense responses, naturally, for language models\n",
        "    the more data and computational power, the better\n",
        "\n",
        "    encoded_text : Complete Encoded Text to make batches from\n",
        "    batch_size : Number of samples per batch\n",
        "    seq_len : Length of character sequence\n",
        "\n",
        "    '''\n",
        "\n",
        "    # Total number of characters per batch\n",
        "    # Example: If samp_per_batch is 2 and seq_len is 50, then 100\n",
        "    # characters come out per batch.\n",
        "    char_per_batch = samp_per_batch * seq_len\n",
        "\n",
        "\n",
        "    # Number of batches available to make\n",
        "    # Use int() to roun to nearest integer\n",
        "    num_batches_avail = int(len(encoded_text)/char_per_batch)\n",
        "\n",
        "    # Cut off end of encoded_text that\n",
        "    # won't fit evenly into a batch\n",
        "    encoded_text = encoded_text[:num_batches_avail * char_per_batch]\n",
        "\n",
        "\n",
        "    # Reshape text into rows the size of a batch\n",
        "    encoded_text = encoded_text.reshape((samp_per_batch, -1))\n",
        "\n",
        "\n",
        "    # Go through each row in array.\n",
        "    for n in range(0, encoded_text.shape[1], seq_len):\n",
        "\n",
        "        # Grab feature characters\n",
        "        x = encoded_text[:, n:n+seq_len]\n",
        "\n",
        "        # y is the target shifted over by 1\n",
        "        y = np.zeros_like(x)\n",
        "\n",
        "        #\n",
        "        try:\n",
        "            y[:, :-1] = x[:, 1:]\n",
        "            y[:, -1]  = encoded_text[:, n+seq_len]\n",
        "\n",
        "        # FOR POTENTIAL INDEXING ERROR AT THE END\n",
        "        except:\n",
        "            y[:, :-1] = x[:, 1:]\n",
        "            y[:, -1] = encoded_text[:, 0]\n",
        "\n",
        "        yield x, y"
      ],
      "metadata": {
        "id": "m6WQfNTQfDP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing the generator of batches for sanity check\n",
        "sample_text = encoded_txt[30:50]\n",
        "sample_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Doufid19rUrD",
        "outputId": "554b757d-75b0-4719-f7d8-d3cdf914e131"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([35, 59, 70, 66, 78, 75, 16, 60, 35, 55, 78, 75, 70, 60, 17, 78, 75,\n",
              "       16, 35, 56])"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_generator = gen_batch(sample_text,samp_per_batch=4,seq_len=5)\n",
        "x, y = next(batch_generator)"
      ],
      "metadata": {
        "id": "90EM43HXrjs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7CsQAoKrpQi",
        "outputId": "3f2711d0-fa22-4bc7-f071-e258b6d16db6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[35, 59, 70, 66, 78],\n",
              "       [75, 16, 60, 35, 55],\n",
              "       [78, 75, 70, 60, 17],\n",
              "       [78, 75, 16, 35, 56]])"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfji59vZrtvB",
        "outputId": "2f1af259-96d2-4ce3-a154-50359b0d129e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[59, 70, 66, 78, 35],\n",
              "       [16, 60, 35, 55, 75],\n",
              "       [75, 70, 60, 17, 78],\n",
              "       [75, 16, 35, 56, 78]])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#now the model architecture, in this case a LSTM\n",
        "class CharModel(nn.Module):\n",
        "\n",
        "    def __init__(self, all_chars, num_hidden=256, num_layers=4,drop_prob=0.5,use_gpu=False):\n",
        "\n",
        "\n",
        "        # SET UP ATTRIBUTES\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.num_layers = num_layers\n",
        "        self.num_hidden = num_hidden\n",
        "        self.use_gpu = use_gpu\n",
        "\n",
        "        #CHARACTER SET, ENCODER, and DECODER\n",
        "        self.all_chars = all_chars\n",
        "        self.decoder = dict(enumerate(all_chars))\n",
        "        self.encoder = {char: ind for ind,char in decoder.items()}\n",
        "\n",
        "\n",
        "        self.lstm = nn.LSTM(len(self.all_chars), num_hidden, num_layers, dropout=drop_prob, batch_first=True)\n",
        "\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "        self.fc_linear = nn.Linear(num_hidden, len(self.all_chars))\n",
        "\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "\n",
        "\n",
        "        lstm_output, hidden = self.lstm(x, hidden)\n",
        "\n",
        "\n",
        "        drop_output = self.dropout(lstm_output)\n",
        "\n",
        "        drop_output = drop_output.contiguous().view(-1, self.num_hidden)\n",
        "\n",
        "\n",
        "        final_out = self.fc_linear(drop_output)\n",
        "\n",
        "\n",
        "        return final_out, hidden\n",
        "\n",
        "\n",
        "    def hidden_state(self, batch_size):\n",
        "        '''\n",
        "        Used as separate method to account for both GPU and CPU users.\n",
        "        '''\n",
        "\n",
        "        if self.use_gpu:\n",
        "\n",
        "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda(),\n",
        "                     torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda())\n",
        "        else:\n",
        "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden),\n",
        "                     torch.zeros(self.num_layers,batch_size,self.num_hidden))\n",
        "\n",
        "        return hidden"
      ],
      "metadata": {
        "id": "C5DA8VnkshJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CharModel(\n",
        "    all_chars=all_chars,\n",
        "    num_hidden=178,\n",
        "    num_layers=3,\n",
        "    drop_prob=0.5,\n",
        "    use_gpu=True,\n",
        ")"
      ],
      "metadata": {
        "id": "jAVMUp7lwIQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_param  = []\n",
        "for p in model.parameters():\n",
        "    total_param.append(int(p.numel()))"
      ],
      "metadata": {
        "id": "v2XvZOg4zxcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(total_param)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lH6kJOztzzAg",
        "outputId": "6a957ca3-e767-4014-f283-d96d36e41156"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "712796"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(encoded_txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mqCFoGLz0qo",
        "outputId": "2a49d898-a967-4ea0-9389-44804484503f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5445609"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "ETAfs47s0mc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now from the text usually 90% is used for training and the rest\n",
        "#is used for testing or validation\n",
        "train_percent = 0.1\n",
        "train_ind = int(len(encoded_txt) * (train_percent))\n",
        "train_data = encoded_txt[:train_ind]\n",
        "val_data = encoded_txt[train_ind:]"
      ],
      "metadata": {
        "id": "6qdZhWkD1G2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now setting parameters for training\n",
        "# Epochs to train for\n",
        "epochs = 100\n",
        "# batch size\n",
        "batch_size = 128\n",
        "\n",
        "# Length of sequence\n",
        "seq_len = 100\n",
        "\n",
        "# for printing report purposes\n",
        "# always start at 0\n",
        "tracker = 0\n",
        "\n",
        "# number of characters in text\n",
        "num_char = max(encoded_txt)+1"
      ],
      "metadata": {
        "id": "sMOnN6rI1hXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set model to train\n",
        "model.train()\n",
        "\n",
        "\n",
        "# Check to see if using GPU\n",
        "if model.use_gpu:\n",
        "    model.cuda()\n",
        "\n",
        "for i in range(epochs):\n",
        "\n",
        "    hidden = model.hidden_state(batch_size)\n",
        "\n",
        "\n",
        "    for x,y in gen_batch(train_data,batch_size,seq_len):\n",
        "\n",
        "        tracker += 1\n",
        "\n",
        "        # One Hot Encode incoming data\n",
        "        x = one_hot_encoder(x,num_char)\n",
        "\n",
        "        # Convert Numpy Arrays to Tensor\n",
        "\n",
        "        inputs = torch.from_numpy(x)\n",
        "        targets = torch.from_numpy(y)\n",
        "\n",
        "        # Adjust for GPU if necessary\n",
        "\n",
        "        if model.use_gpu:\n",
        "\n",
        "            inputs = inputs.cuda()\n",
        "            targets = targets.cuda()\n",
        "\n",
        "        # Reset Hidden State\n",
        "        # If we dont' reset we would backpropagate through all training history\n",
        "        hidden = tuple([state.data for state in hidden])\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        lstm_output, hidden = model.forward(inputs,hidden)\n",
        "        loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # POSSIBLE EXPLODING GRADIENT PROBLEM!\n",
        "        # LET\"S CLIP JUST IN CASE\n",
        "        nn.utils.clip_grad_norm_(model.parameters(),max_norm=5)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "        ###################################\n",
        "        ### CHECK ON VALIDATION SET ######\n",
        "        #################################\n",
        "\n",
        "        if tracker % 25 == 0:\n",
        "\n",
        "            val_hidden = model.hidden_state(batch_size)\n",
        "            val_losses = []\n",
        "            model.eval()\n",
        "\n",
        "            for x,y in gen_batch(val_data,batch_size,seq_len):\n",
        "\n",
        "                # One Hot Encode incoming data\n",
        "                x = one_hot_encoder(x,num_char)\n",
        "\n",
        "\n",
        "                # Convert Numpy Arrays to Tensor\n",
        "\n",
        "                inputs = torch.from_numpy(x)\n",
        "                targets = torch.from_numpy(y)\n",
        "\n",
        "                # Adjust for GPU if necessary\n",
        "\n",
        "                if model.use_gpu:\n",
        "\n",
        "                    inputs = inputs.cuda()\n",
        "                    targets = targets.cuda()\n",
        "\n",
        "                # Reset Hidden State\n",
        "                # If we dont' reset we would backpropagate through\n",
        "                # all training history\n",
        "                val_hidden = tuple([state.data for state in val_hidden])\n",
        "\n",
        "                lstm_output, val_hidden = model.forward(inputs,val_hidden)\n",
        "                val_loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
        "\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "            # Reset to training model after val for loop\n",
        "            model.train()\n",
        "\n",
        "            print(f\"Epoch: {i} Step: {tracker} Val Loss: {val_loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfrl-5hwdN43",
        "outputId": "b1a8434c-5e67-4297-d52e-a257c31ce5cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Step: 25 Val Loss: 3.2577576637268066\n",
            "Epoch: 1 Step: 50 Val Loss: 3.237203598022461\n",
            "Epoch: 1 Step: 75 Val Loss: 3.2370030879974365\n",
            "Epoch: 2 Step: 100 Val Loss: 3.2355527877807617\n",
            "Epoch: 2 Step: 125 Val Loss: 3.234680414199829\n",
            "Epoch: 3 Step: 150 Val Loss: 3.2341949939727783\n",
            "Epoch: 4 Step: 175 Val Loss: 3.2315306663513184\n",
            "Epoch: 4 Step: 200 Val Loss: 3.2326951026916504\n",
            "Epoch: 5 Step: 225 Val Loss: 3.1943657398223877\n",
            "Epoch: 5 Step: 250 Val Loss: 3.109851360321045\n",
            "Epoch: 6 Step: 275 Val Loss: 3.047459125518799\n",
            "Epoch: 7 Step: 300 Val Loss: 2.9785919189453125\n",
            "Epoch: 7 Step: 325 Val Loss: 2.887070417404175\n",
            "Epoch: 8 Step: 350 Val Loss: 2.815244197845459\n",
            "Epoch: 8 Step: 375 Val Loss: 2.74607515335083\n",
            "Epoch: 9 Step: 400 Val Loss: 2.667167901992798\n",
            "Epoch: 10 Step: 425 Val Loss: 2.6020984649658203\n",
            "Epoch: 10 Step: 450 Val Loss: 2.5435638427734375\n",
            "Epoch: 11 Step: 475 Val Loss: 2.492096424102783\n",
            "Epoch: 11 Step: 500 Val Loss: 2.448340892791748\n",
            "Epoch: 12 Step: 525 Val Loss: 2.4083921909332275\n",
            "Epoch: 13 Step: 550 Val Loss: 2.3776538372039795\n",
            "Epoch: 13 Step: 575 Val Loss: 2.3439507484436035\n",
            "Epoch: 14 Step: 600 Val Loss: 2.3192505836486816\n",
            "Epoch: 14 Step: 625 Val Loss: 2.2987678050994873\n",
            "Epoch: 15 Step: 650 Val Loss: 2.278090715408325\n",
            "Epoch: 16 Step: 675 Val Loss: 2.2667031288146973\n",
            "Epoch: 16 Step: 700 Val Loss: 2.2497591972351074\n",
            "Epoch: 17 Step: 725 Val Loss: 2.2390921115875244\n",
            "Epoch: 17 Step: 750 Val Loss: 2.2264161109924316\n",
            "Epoch: 18 Step: 775 Val Loss: 2.218825578689575\n",
            "Epoch: 19 Step: 800 Val Loss: 2.205626964569092\n",
            "Epoch: 19 Step: 825 Val Loss: 2.1955618858337402\n",
            "Epoch: 20 Step: 850 Val Loss: 2.190418004989624\n",
            "Epoch: 20 Step: 875 Val Loss: 2.1766862869262695\n",
            "Epoch: 21 Step: 900 Val Loss: 2.1709911823272705\n",
            "Epoch: 22 Step: 925 Val Loss: 2.1570701599121094\n",
            "Epoch: 22 Step: 950 Val Loss: 2.1514692306518555\n",
            "Epoch: 23 Step: 975 Val Loss: 2.14788556098938\n",
            "Epoch: 23 Step: 1000 Val Loss: 2.1325347423553467\n",
            "Epoch: 24 Step: 1025 Val Loss: 2.12760066986084\n",
            "Epoch: 24 Step: 1050 Val Loss: 2.119420289993286\n",
            "Epoch: 25 Step: 1075 Val Loss: 2.112248420715332\n",
            "Epoch: 26 Step: 1100 Val Loss: 2.1174213886260986\n",
            "Epoch: 26 Step: 1125 Val Loss: 2.099616289138794\n",
            "Epoch: 27 Step: 1150 Val Loss: 2.0939388275146484\n",
            "Epoch: 27 Step: 1175 Val Loss: 2.0885891914367676\n",
            "Epoch: 28 Step: 1200 Val Loss: 2.0837531089782715\n",
            "Epoch: 29 Step: 1225 Val Loss: 2.074024200439453\n",
            "Epoch: 29 Step: 1250 Val Loss: 2.0711889266967773\n",
            "Epoch: 30 Step: 1275 Val Loss: 2.0703608989715576\n",
            "Epoch: 30 Step: 1300 Val Loss: 2.0609586238861084\n",
            "Epoch: 31 Step: 1325 Val Loss: 2.0552496910095215\n",
            "Epoch: 32 Step: 1350 Val Loss: 2.0496480464935303\n",
            "Epoch: 32 Step: 1375 Val Loss: 2.046846628189087\n",
            "Epoch: 33 Step: 1400 Val Loss: 2.0412964820861816\n",
            "Epoch: 33 Step: 1425 Val Loss: 2.0400426387786865\n",
            "Epoch: 34 Step: 1450 Val Loss: 2.033889055252075\n",
            "Epoch: 35 Step: 1475 Val Loss: 2.0288805961608887\n",
            "Epoch: 35 Step: 1500 Val Loss: 2.0250608921051025\n",
            "Epoch: 36 Step: 1525 Val Loss: 2.0238192081451416\n",
            "Epoch: 36 Step: 1550 Val Loss: 2.0156502723693848\n",
            "Epoch: 37 Step: 1575 Val Loss: 2.0125741958618164\n",
            "Epoch: 38 Step: 1600 Val Loss: 2.005948066711426\n",
            "Epoch: 38 Step: 1625 Val Loss: 2.0038719177246094\n",
            "Epoch: 39 Step: 1650 Val Loss: 2.003216028213501\n",
            "Epoch: 39 Step: 1675 Val Loss: 1.9944437742233276\n",
            "Epoch: 40 Step: 1700 Val Loss: 1.9948675632476807\n",
            "Epoch: 41 Step: 1725 Val Loss: 1.9893264770507812\n",
            "Epoch: 41 Step: 1750 Val Loss: 1.9857097864151\n",
            "Epoch: 42 Step: 1775 Val Loss: 1.9855042695999146\n",
            "Epoch: 42 Step: 1800 Val Loss: 1.9779106378555298\n",
            "Epoch: 43 Step: 1825 Val Loss: 1.9749904870986938\n",
            "Epoch: 44 Step: 1850 Val Loss: 1.9721152782440186\n",
            "Epoch: 44 Step: 1875 Val Loss: 1.9684672355651855\n",
            "Epoch: 45 Step: 1900 Val Loss: 1.9729080200195312\n",
            "Epoch: 45 Step: 1925 Val Loss: 1.959162950515747\n",
            "Epoch: 46 Step: 1950 Val Loss: 1.961016058921814\n",
            "Epoch: 47 Step: 1975 Val Loss: 1.9561690092086792\n",
            "Epoch: 47 Step: 2000 Val Loss: 1.9566856622695923\n",
            "Epoch: 48 Step: 2025 Val Loss: 1.9555882215499878\n",
            "Epoch: 48 Step: 2050 Val Loss: 1.9524198770523071\n",
            "Epoch: 49 Step: 2075 Val Loss: 1.9458547830581665\n",
            "Epoch: 49 Step: 2100 Val Loss: 1.9436557292938232\n",
            "Epoch: 50 Step: 2125 Val Loss: 1.9444407224655151\n",
            "Epoch: 51 Step: 2150 Val Loss: 1.9404085874557495\n",
            "Epoch: 51 Step: 2175 Val Loss: 1.9363049268722534\n",
            "Epoch: 52 Step: 2200 Val Loss: 1.9402614831924438\n",
            "Epoch: 52 Step: 2225 Val Loss: 1.9293999671936035\n",
            "Epoch: 53 Step: 2250 Val Loss: 1.9327374696731567\n",
            "Epoch: 54 Step: 2275 Val Loss: 1.9231892824172974\n",
            "Epoch: 54 Step: 2300 Val Loss: 1.925872802734375\n",
            "Epoch: 55 Step: 2325 Val Loss: 1.925202488899231\n",
            "Epoch: 55 Step: 2350 Val Loss: 1.9193273782730103\n",
            "Epoch: 56 Step: 2375 Val Loss: 1.9237678050994873\n",
            "Epoch: 57 Step: 2400 Val Loss: 1.9137569665908813\n",
            "Epoch: 57 Step: 2425 Val Loss: 1.9137413501739502\n",
            "Epoch: 58 Step: 2450 Val Loss: 1.9127464294433594\n",
            "Epoch: 58 Step: 2475 Val Loss: 1.9063044786453247\n",
            "Epoch: 59 Step: 2500 Val Loss: 1.9059600830078125\n",
            "Epoch: 60 Step: 2525 Val Loss: 1.9045888185501099\n",
            "Epoch: 60 Step: 2550 Val Loss: 1.905422329902649\n",
            "Epoch: 61 Step: 2575 Val Loss: 1.903733253479004\n",
            "Epoch: 61 Step: 2600 Val Loss: 1.901247501373291\n",
            "Epoch: 62 Step: 2625 Val Loss: 1.8998507261276245\n",
            "Epoch: 63 Step: 2650 Val Loss: 1.8967210054397583\n",
            "Epoch: 63 Step: 2675 Val Loss: 1.8967337608337402\n",
            "Epoch: 64 Step: 2700 Val Loss: 1.8978776931762695\n",
            "Epoch: 64 Step: 2725 Val Loss: 1.8896560668945312\n",
            "Epoch: 65 Step: 2750 Val Loss: 1.8966963291168213\n",
            "Epoch: 66 Step: 2775 Val Loss: 1.8880184888839722\n",
            "Epoch: 66 Step: 2800 Val Loss: 1.8900643587112427\n",
            "Epoch: 67 Step: 2825 Val Loss: 1.8896362781524658\n",
            "Epoch: 67 Step: 2850 Val Loss: 1.8847575187683105\n",
            "Epoch: 68 Step: 2875 Val Loss: 1.8864022493362427\n",
            "Epoch: 69 Step: 2900 Val Loss: 1.8822760581970215\n",
            "Epoch: 69 Step: 2925 Val Loss: 1.8841509819030762\n",
            "Epoch: 70 Step: 2950 Val Loss: 1.8782402276992798\n",
            "Epoch: 70 Step: 2975 Val Loss: 1.879533052444458\n",
            "Epoch: 71 Step: 3000 Val Loss: 1.879837989807129\n",
            "Epoch: 72 Step: 3025 Val Loss: 1.8741555213928223\n",
            "Epoch: 72 Step: 3050 Val Loss: 1.878208041191101\n",
            "Epoch: 73 Step: 3075 Val Loss: 1.8723560571670532\n",
            "Epoch: 73 Step: 3100 Val Loss: 1.8721638917922974\n",
            "Epoch: 74 Step: 3125 Val Loss: 1.8738430738449097\n",
            "Epoch: 74 Step: 3150 Val Loss: 1.8717344999313354\n",
            "Epoch: 75 Step: 3175 Val Loss: 1.8710761070251465\n",
            "Epoch: 76 Step: 3200 Val Loss: 1.866325855255127\n",
            "Epoch: 76 Step: 3225 Val Loss: 1.8733735084533691\n",
            "Epoch: 77 Step: 3250 Val Loss: 1.8708311319351196\n",
            "Epoch: 77 Step: 3275 Val Loss: 1.87139093875885\n",
            "Epoch: 78 Step: 3300 Val Loss: 1.8670579195022583\n",
            "Epoch: 79 Step: 3325 Val Loss: 1.862678050994873\n",
            "Epoch: 79 Step: 3350 Val Loss: 1.8689398765563965\n",
            "Epoch: 80 Step: 3375 Val Loss: 1.8716219663619995\n",
            "Epoch: 80 Step: 3400 Val Loss: 1.8617974519729614\n",
            "Epoch: 81 Step: 3425 Val Loss: 1.8660327196121216\n",
            "Epoch: 82 Step: 3450 Val Loss: 1.8622664213180542\n",
            "Epoch: 82 Step: 3475 Val Loss: 1.8616549968719482\n",
            "Epoch: 83 Step: 3500 Val Loss: 1.863236665725708\n",
            "Epoch: 83 Step: 3525 Val Loss: 1.8560599088668823\n",
            "Epoch: 84 Step: 3550 Val Loss: 1.8613650798797607\n",
            "Epoch: 85 Step: 3575 Val Loss: 1.8530495166778564\n",
            "Epoch: 85 Step: 3600 Val Loss: 1.8548264503479004\n",
            "Epoch: 86 Step: 3625 Val Loss: 1.856929898262024\n",
            "Epoch: 86 Step: 3650 Val Loss: 1.8497068881988525\n",
            "Epoch: 87 Step: 3675 Val Loss: 1.8590757846832275\n",
            "Epoch: 88 Step: 3700 Val Loss: 1.8501919507980347\n",
            "Epoch: 88 Step: 3725 Val Loss: 1.8547368049621582\n",
            "Epoch: 89 Step: 3750 Val Loss: 1.8516550064086914\n",
            "Epoch: 89 Step: 3775 Val Loss: 1.849554181098938\n",
            "Epoch: 90 Step: 3800 Val Loss: 1.8597171306610107\n",
            "Epoch: 91 Step: 3825 Val Loss: 1.8459124565124512\n",
            "Epoch: 91 Step: 3850 Val Loss: 1.8562343120574951\n",
            "Epoch: 92 Step: 3875 Val Loss: 1.8488634824752808\n",
            "Epoch: 92 Step: 3900 Val Loss: 1.8518353700637817\n",
            "Epoch: 93 Step: 3925 Val Loss: 1.8495548963546753\n",
            "Epoch: 94 Step: 3950 Val Loss: 1.8504191637039185\n",
            "Epoch: 94 Step: 3975 Val Loss: 1.8542689085006714\n",
            "Epoch: 95 Step: 4000 Val Loss: 1.8473937511444092\n",
            "Epoch: 95 Step: 4025 Val Loss: 1.850083827972412\n",
            "Epoch: 96 Step: 4050 Val Loss: 1.8496267795562744\n",
            "Epoch: 97 Step: 4075 Val Loss: 1.8448125123977661\n",
            "Epoch: 97 Step: 4100 Val Loss: 1.8511558771133423\n",
            "Epoch: 98 Step: 4125 Val Loss: 1.8459337949752808\n",
            "Epoch: 98 Step: 4150 Val Loss: 1.8488165140151978\n",
            "Epoch: 99 Step: 4175 Val Loss: 1.8729435205459595\n",
            "Epoch: 99 Step: 4200 Val Loss: 1.8526393175125122\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'book1.net'\n",
        "torch.save(model.state_dict(),model_name)"
      ],
      "metadata": {
        "id": "yMxBI86CebN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CharModel(\n",
        "    all_chars=all_chars,\n",
        "    num_hidden=178,\n",
        "    num_layers=3,\n",
        "    drop_prob=0.5,\n",
        "    use_gpu=True,\n",
        ")"
      ],
      "metadata": {
        "id": "uMpqkPFqeiMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(model_name))\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCrej232gVOH",
        "outputId": "72041a11-d1c8-48af-d23f-bac968d649e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CharModel(\n",
              "  (lstm): LSTM(84, 178, num_layers=3, batch_first=True, dropout=0.5)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (fc_linear): Linear(in_features=178, out_features=84, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#now it's time to generate some predictions\n",
        "def predict_next_char(model, char, hidden=None, k=1):\n",
        "\n",
        "        # Encode raw letters with model\n",
        "        encoded_text = model.encoder[char]\n",
        "\n",
        "        # set as numpy array for one hot encoding\n",
        "        # NOTE THE [[ ]] dimensions!!\n",
        "        encoded_text = np.array([[encoded_text]])\n",
        "\n",
        "        # One hot encoding\n",
        "        encoded_text = one_hot_encoder(encoded_text, len(model.all_chars))\n",
        "\n",
        "        # Convert to Tensor\n",
        "        inputs = torch.from_numpy(encoded_text)\n",
        "\n",
        "        # Check for CPU\n",
        "        if(model.use_gpu):\n",
        "            inputs = inputs.cuda()\n",
        "\n",
        "\n",
        "        # Grab hidden states\n",
        "        hidden = tuple([state.data for state in hidden])\n",
        "\n",
        "\n",
        "        # Run model and get predicted output\n",
        "        lstm_out, hidden = model(inputs, hidden)\n",
        "\n",
        "\n",
        "        # Convert lstm_out to probabilities\n",
        "        probs = F.softmax(lstm_out, dim=1).data\n",
        "\n",
        "\n",
        "\n",
        "        if(model.use_gpu):\n",
        "            # move back to CPU to use with numpy\n",
        "            probs = probs.cpu()\n",
        "\n",
        "\n",
        "        # k determines how many characters to consider\n",
        "        # for our probability choice.\n",
        "        # https://pytorch.org/docs/stable/torch.html#torch.topk\n",
        "\n",
        "        # Return k largest probabilities in tensor\n",
        "        probs, index_positions = probs.topk(k)\n",
        "\n",
        "\n",
        "        index_positions = index_positions.numpy().squeeze()\n",
        "\n",
        "        # Create array of probabilities\n",
        "        probs = probs.numpy().flatten()\n",
        "\n",
        "        # Convert to probabilities per index\n",
        "        probs = probs/probs.sum()\n",
        "\n",
        "        # randomly choose a character based on probabilities\n",
        "        char = np.random.choice(index_positions, p=probs)\n",
        "\n",
        "        # return the encoded value of the predicted char and the hidden state\n",
        "        return model.decoder[char], hidden"
      ],
      "metadata": {
        "id": "GC816yGogaVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, size, seed='The', k=1):\n",
        "\n",
        "\n",
        "\n",
        "    # CHECK FOR GPU\n",
        "    if(model.use_gpu):\n",
        "        model.cuda()\n",
        "    else:\n",
        "        model.cpu()\n",
        "\n",
        "    # Evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # begin output from initial seed\n",
        "    output_chars = [c for c in seed]\n",
        "\n",
        "    # intiate hidden state\n",
        "    hidden = model.hidden_state(1)\n",
        "\n",
        "    # predict the next character for every character in seed\n",
        "    for char in seed:\n",
        "        char, hidden = predict_next_char(model, char, hidden, k=k)\n",
        "\n",
        "    # add initial characters to output\n",
        "    output_chars.append(char)\n",
        "\n",
        "    # Now generate for size requested\n",
        "    for i in range(size):\n",
        "\n",
        "        # predict based off very last letter in output_chars\n",
        "        char, hidden = predict_next_char(model, output_chars[-1], hidden, k=k)\n",
        "\n",
        "        # add predicted character\n",
        "        output_chars.append(char)\n",
        "\n",
        "    # return string of predicted text\n",
        "    return ''.join(output_chars)"
      ],
      "metadata": {
        "id": "gR0eLHOUhqR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, 1000, seed='She ', k=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "vAVVOgQPhtC_",
        "outputId": "d5500d79-0003-4423-c28b-4984c2d3d7d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'generate_text' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-46bcb94060da>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'She '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'generate_text' is not defined"
          ]
        }
      ]
    }
  ]
}